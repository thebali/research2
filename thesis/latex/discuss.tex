\chapter{Discussion}
As stated in the introduction, this thesis aims to evaluate the performance
of Multi-Armed Bandit Algorithms in an e-commerce environment. In this
chapter evaluation of user-experience and comparisons with other algorithms
will be presented together with thoughts regarding the result and future work.

A
7.1

Evaluating The Result

The result can be evaluated depending on several different aspects. In this section focus
will be put on evaluation of performance and user-experience as well as some thoughts
regarding why the result turned out the way it did.

7.1.1

Performance

The percentage of correct predictions can be seen on the y-axis of the graphs in section
6.1. By comparing the graphs in figure 6.1 one can draw the conclusion that the best
recommendations are made in the time interval between one week to one month posterior to starting the application. Predictions in this period got an accuracy of 19 to 21
percent. Seeing how this is in a garment-based e-commerce environment it makes sense
that new collections of products are added every other month, whereas the products from
last month’s collection do not sell as good. The simulation made in these graphs starts
in late spring, 2012-05-01, meaning that recommendations of cloths for the summer are
more likely to be of satisfactory than a few months later when summer-based cloths are
recommended for the fall or winter. The conclusion here is that if this application was
to run in a live system, it would make sense to reset the trained data prior to releasing
a new collection and/or before each new season.
As mentioned earlier, the dataset used in this thesis contains only static data. Meaning
that for a prediction to be seen as correct, a recommended item has to be purchased at
38

7.1. EVALUATION

CHAPTER 7. DISCUSSION

some point in time after the recommendation is performed. The problem using static
data rather than testing an application live in this setting is that possibly good recommendations are not measured, because an item was not purchased. One can never know
if the user would have bought more products if products that she could possibly like
would be presented to her. Hence the only thing that can be measured is actual purchases, while in theory a user is likely to buy more products based on the recommended
items. Something that could be said to be certain about the application here is that it
predicts correct items with at least 19-21 percent accuracy and probably higher in most
cases.
All the graphs in Chapter 6 expressing anonymous data to some extent, proves that
taking a more privacy-preserving approach, actually gives worse performance in terms
of measuring successful recommendations. By comparing the graphs in figure 6.1 where
the algorithm is using all available context, with the other graphs in figure 6.2 where
the algorithm is using less context, it is possible to see a difference of about 6-10 percent
depending on which graphs are observed. It is hard to tell if it is worth it or not, but
seeing how things work in reality where most people accept any license agreement of applications or web-pages, one can draw the conclusion that as long as the observed data
is used in ways that benefit the actual user and not a third party, it is at least socially
acceptable. Facebook is a good example of this, which in their licence agreement [63]
state that they have all the rights to any data you submit or take part of online, when
using their services. They also state that your data will be accessible by companies and
third parties of their choosing.
There are also other aspects to a good result than through only measuring the number of
successful recommendations, such as user experience and satisfaction level of customers.
Unfortunately these are hard to measure without deploying the system live. See more
about user-experience under 7.1.2.
So far we have seen that the implemented application can predict shopping behaviour
of most users with a probability of around 20 percent. So the question is how well
does other similar applications perform on similar datasets. One similar application in
a similar setting can be seen in [61] where they use significantly different algorithms and
achieve correct recommendations with a probability of about 17 percent. The authors
of [61] make use of a collaborative filtering-based techniques, but with significantly better performance than the collaborative filtering-based algorithms used in this project.
Nevertheless, the content-based filtering implementation using Contextual Multi-Armed
Bandits in this thesis outperforms the algorithms used in [61], something that ought to
be considered as successful. In figure 7.1 below, the different implemented algorithms
of this thesis is presented with different colours, using the time interval of one month
because of its relevance as described above.

39

7.1. EVALUATION

CHAPTER 7. DISCUSSION

Predictions made of data from 1 month in time
25
Best Arm
All Arms
Popularity(Baseline)
Jaccard Similarity

Correct predictions (%)

20

15

10

5

0

0

2000

4000

6000

8000

10000

12000

14000

16000

Number of predictions

Figure 7.1: Graph showing the results of all implemented algorithms from one month in
time

And as can be seen in this graph, the Contextual Multi-Armed Bandit Algorithm outperforms all of the others. Notable is also that the popularity baseline algorithm appears
to have pretty high performance. Something that is easily realised why it is the case.
This strategy is very bad in other aspects as if taking User-Experience into account. See
section 7.1.2 for more details regarding this.
The next graph, in figure 7.2, shows the performance of all algorithms in different colours,
measuring performance of purchases over one year.

40

18000

7.1. EVALUATION

CHAPTER 7. DISCUSSION

Predictions made of data from 12 months in time
30
Best Arm
All Arms
Popularity(Baseline)
Jaccard Similarity

Correct predictions (%)

25

20

15

10

5

0

0

0.5

1

1.5

2

2.5

3

Number of predictions

3.5
×105

Figure 7.2: Graph showing the results of all implemented algorithms from one year in time

Here one can see that the result does not change even in the long run. All algorithms
perform best during the first months due to reasons expressed in the beginning of this
section, but even after a long time the order of good versus bad algorithms do not change.
Something notable is what happens in the time span between 1 and 3 months, where
we can see a dip in performance in terms of successful recommendations. The graph
stretches from 2012-05-01 to 2012-07-31. In this time period there is a shift in products
wanted by the users. A user that has bought for instance some pairs of jeans during
May has a high probability of being recommended jeans during the later months too.
However, during the summer it is more likely that the user wants to buy shorts and other
products more suitable for use during the summer. As can be seen in graph 7.2, the popularity curve also become worse during these months. This means that user-behaviour
is harder to predict during these months. This might be the case either because there is
a larger set of items that can be recommended, or because user behaviour is not as predictable during the summer as during the spring. As mentioned initially in this chapter,
it is also likely that new collections of garment are released before each season, which
might be another contributing factor to the dip in performance during the specific time
spans.
One could wonder: why did the collaborative filtering-based algorithm using Jaccard

41

7.1. EVALUATION

CHAPTER 7. DISCUSSION

Similarity perform so poorly with the Junkyard dataset, in comparison with the other
algorithms in this thesis and the ones described in [61]. This, of course, might depend on
a variety of different reasons such as algorithms or datasets being significantly different.
Seeing how the focus of this evaluation is on the performance of Multi-Armed Bandits,
further investigation of the poor result of the above mentioned will be left for future work.
As explained in 3.2, regret bounds are often used for measuring negative emotion experience in decision theory and probabilistic modelling. In figure 7.3 below, the total regret
is illustrated for ’All Arms’, ’Most Buys’ and ’Jaccard Similarity’ relative to the ’Best
Arm’.
Regret of All Arms, Popularity and Jaccard Similarity relative to the Best Arm

100

All Arms
Popularity(Baseline)
Jaccard Similarity

90

80

Regret (%)

70

60

50

40

30

20

10

0

2000

4000

6000

8000

10000

12000

14000

16000

Number of predictions

Figure 7.3: Regret of ’All Arms’, ’Most Buys’ and ’Jaccard Similarity’ relative to the Best
Arm.

An interesting scenario that was stumbled upon is if the following happens (7.4):

42

18000

7.1. EVALUATION

CHAPTER 7. DISCUSSION

Predictions made of data from 1 month in time
20
Best Arm
All Arms

18
16

Correct predictions (%)

14
12
10
8
6
4
2
0

0

1000

2000

3000

4000

5000

6000

Number of predictions

Figure 7.4: All Arms outperforming Best Arm for some time

What can be seen here is running the algorithm for six months, and where All Arms
seemingly perform better than the Best Arm at some point in time. There is a simple
explanation to this phenomenon. Posterior to running the algorithm, the algorithm looks
at all arms to see which one that is having the best performance. Thus the arm with
the best performance posterior to running the algorithm is not necessarily the best arm
throughout the whole simulation.

7.1.2

User-Experience

User-Experience is more tricky to measure than performance. Despite suggesting items
that are likely to be bought statistically, it does not mean that it will make a specific
customer happy. Recommending items that might be of no value for a specific customer,
but are best-sellers, is unarguably not the best way to present recommendations even
if it will show good performance. This can be seen in the graphs in figure 6.3 where
recommendations are made entirely based on whats the currently most bought items as
of the last week.

43

7000

7.1. EVALUATION

CHAPTER 7. DISCUSSION

Like a physical store
When visiting a physical store, you would not want the shopping assistant to only propose the same items as she is proposing to everyone else. For instance by proposing only
the best-sellers as of the last week, not taking anything else into consideration. Instead
you would want the shopping assistant to listen to what kind of items you like and have
enjoyed earlier, and what you are currently looking for. The same thing applies for most
people when browsing E-Commerce websites or platforms. If you browse for or buy certain items, the system should adapt to this and actually try to help you find what you
might be looking for in the future. That is if you are looking to buy a pair of shoes and
you are recommended ten different shirts, just because the shirts happened to be on sale
last week meaning that a lot of people have purchased them, you will not be satisfied or
any closer to finding the pair of shoes you were after.
If however, let us say, the shopping assistant was to follow you around noting what
you were looking at or even what you were buying in other stores, you would probably
find her creepy and probably stay away from her stores. This despite the fact that it
might mean ending up with an item you enjoy, faster. Once again, the same idea can
be applied in E-Commerce systems; using all data you can from a specific user is not
ethically acceptable. It is however more socially acceptable as described in 7.1.1. This
could be data such as cookie-based history of other browsed web sites. It could also be
data retrieved from buying and selling information gathered in other systems.
Our implementation
When implementing the algorithm in this project, the physical store was kept in mind.
Users should not relate to the system as the creepy shopping assistant. The implemented
application thus only focus on using data gathered from the platform which it was built
for and even there discussing different privacy-preserving aspects. Neither should users
relate to the system as the lazy shopping assistant who only gives the same recommendations to everyone. Instead focus was put on making the customers feel that the system or
shopping assistant actually proposed valid items that the customer could be interested
in. This by giving ten recommendations that match the specific customer’s profile.
Taking it live
Observing the shopping history of a user in the store is an intuitively good technique to
figure out what the user is interested in. However, if taking the system live,predictions
would also based on the current live session. The taste of a user would probably most
often be similar from the different times she visits the website, but she might want to
shop from different categories during different sessions. Looking for shoes at one time
should not mean that the user only gets shoe recommendations the second time she
browses the site. If the user however only looks for gender-specific items, most of the
time there would be no point in suggesting the opposite.

44

7.2. FUTURE WORK

CHAPTER 7. DISCUSSION

Best-seller recommendations are not useless in any way. Using this application in a
live setting could mean using best-seller recommendations on the front page so that
people occasionally browsing the page in hopes of finding something they like might get
lucky, while the personalised recommendations could be used on the specific user-pages.

7.2

Future Work

There are still issues related with the result of this project that could be desirable to
address, but that goes out of the scope of this thesis. In this section we will present
some concrete ideas of possible future work related to this thesis.

7.2.1

Synthetic Data

Synthetic data is, as opposed to authentic data, generated within some behavioural
model. It is explained in [64] and described as: ”Synthetic data can be defined as data
that are generated by simulated users in a simulated system, performing simulated actions.
One obvious use-area for synthetic data is the possibility of doing realistic testing of
a system or parts of a system before deploying them live. There are however many difficulties that need to be considered before implementation can start. Difficulties that are
not entirely addressed in current research as it varies in different settings and systems.

7.2.2

Extensive Collaborative Filtering-Based Algorithm

The implemented collaborative filtering-based algorithm in this thesis perform poorly in
comparison to what current research expresses when describing collaborative filteringbased algorithms. It would be interesting to make use of different similarity computing
techniques to see if it could, first of all, provide a better result than the implemented
collaborative filtering-based algorithm in this thesis, but also if it could beat the performance of the Contextual Multi-Armed Bandit Algorithm implemented in this thesis.

45

8
Conclusion
n this thesis the performance of Contextual Multi-Armed Bandit Algorithms have
been measured and evaluated in terms of successful recommendations, user-experience
and possibility of working using privacy-preserving methods, in a garment-based
e-commerce environment. The result can be seen in chapter 6. In the aspect of making successful recommendations and giving a good user-experience the results look good.
But also in the cases where privacy-preserving approaches have been taken, the results
are fairly good. Depending on the setting and system, if required, it would thereby be
feasible to consider even the privacy-preserving modes of the algorithm as performance
in terms of successful recommendations are fairly good despite those restrictions.

I

8.1

The Research Question

In the introduction we defined the research question as:
Would the algorithmic approach of Contextual Multi-Armed Bandit Algorithms perform
well enough to belong among the different algorithmic approaches to consider when implementing recommender systems?
A justified answer to this question, with respect to the results presented in Chapter 6 and
the reasoning of the results presented in Chapter 7, is ”yes”. A reasonable conclusion that
can be made from observing the result where purchasing behaviour is predicted with a
probability of over 20 % is that considering Contextual Multi-Armed Bandit Algorithms
when implementing recommender systems is highly appropriate.
The more complex but also most suitable answer to the research question is ”probably”
or ”it depends on the setting and environment”. As the name Contextual Multi-Armed
Bandit Algorithms indicates, the algorithm makes use of contexts such as a user’s context, personal information, and might thus not be eligible in systems where all data is to
be totally anonymous or where only non-personal data exists. However, as can be seen

