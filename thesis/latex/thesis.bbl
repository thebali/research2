% $ biblatex auxiliary file $
% $ biblatex bbl format version 2.9 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{none/global//global/global}
    \entry{Park2012}{article}{}
      \name{author}{4}{}{%
        {{hash=5b745df0961f7e29675185b7e9258a44}{%
           family={Park},
           familyi={P\bibinitperiod},
           given={Deuk\bibnamedelima Hee},
           giveni={D\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=4e296c0f6c7f55ce8f119caa77ac5636}{%
           family={Kim},
           familyi={K\bibinitperiod},
           given={Hyea\bibnamedelima Kyeong},
           giveni={H\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=2168c8f0e63b7f61dc342105016ff52f}{%
           family={Choi},
           familyi={C\bibinitperiod},
           given={Il\bibnamedelima Young},
           giveni={I\bibinitperiod\bibinitdelim Y\bibinitperiod}}}%
        {{hash=ed47f99ac8789a656edb6c8e7686d7a1}{%
           family={Kim},
           familyi={K\bibinitperiod},
           given={Jae\bibnamedelima Kyeong},
           giveni={J\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Elsevier Ltd}%
      }
      \strng{namehash}{5ff214b4297f096d8b8d44b3a25d1d87}
      \strng{fullhash}{e68648e54839c0c44bbb9887351232d2}
      \strng{bibnamehash}{5ff214b4297f096d8b8d44b3a25d1d87}
      \strng{authorbibnamehash}{5ff214b4297f096d8b8d44b3a25d1d87}
      \strng{authornamehash}{5ff214b4297f096d8b8d44b3a25d1d87}
      \strng{authorfullhash}{e68648e54839c0c44bbb9887351232d2}
      \field{sortinit}{1}
      \field{sortinithash}{2174f786c6195e7fe2ee1c229b416e29}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recommender systems have become an important research field since the emergence of the first paper on collaborative filtering in the mid-1990s. Although academic research on recommender systems has increased significantly over the past 10 years, there are deficiencies in the comprehensive literature review and classification of that research. For that reason, we reviewed 210 articles on recommender systems from 46 journals published between 2001 and 2010, and then classified those by the year of publication, the journals in which they appeared, their application fields, and their data mining techniques. The 210 articles are categorized into eight application fields (books, documents, images, movie, music, shopping, TV programs, and others) and eight data mining techniques (association rule, clustering, decision tree, k-nearest neighbor, link analysis, neural network, regression, and other heuristic methods). Our research provides information about trends in recommender systems research by examining the publication years of the articles, and provides practitioners and researchers with insight and future direction on recommender systems. We hope that this paper helps anyone who is interested in recommender systems research with insight for future research direction. {©} 2012 Elsevier Ltd. All rights reserved.}
      \field{isbn}{09574174}
      \field{issn}{09574174}
      \field{journaltitle}{Expert Systems with Applications}
      \field{number}{11}
      \field{title}{{A literature review and classification of recommender systems research}}
      \field{volume}{39}
      \field{year}{2012}
      \field{pages}{10059\bibrangedash 10072}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1016/j.eswa.2012.02.038
      \endverb
      \verb{urlraw}
      \verb http://dx.doi.org/10.1016/j.eswa.2012.02.038
      \endverb
      \verb{url}
      \verb http://dx.doi.org/10.1016/j.eswa.2012.02.038
      \endverb
      \keyw{Classification,Data mining technique,Literature review,Recommender systems}
    \endentry
    \entry{Wilbur1992}{article}{}
      \name{author}{2}{}{%
        {{hash=37556444561be148ae5a79fb2c40f338}{%
           family={Wilbur},
           familyi={W\bibinitperiod},
           given={W.\bibnamedelimi John},
           giveni={W\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=a4e468755ab07dede54e21de210b128b}{%
           family={Sirotkin},
           familyi={S\bibinitperiod},
           given={Karl},
           giveni={K\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Sage Publications, Inc.}%
      }
      \strng{namehash}{750cc371a8ded9f79a45dbc5e88df115}
      \strng{fullhash}{750cc371a8ded9f79a45dbc5e88df115}
      \strng{bibnamehash}{750cc371a8ded9f79a45dbc5e88df115}
      \strng{authorbibnamehash}{750cc371a8ded9f79a45dbc5e88df115}
      \strng{authornamehash}{750cc371a8ded9f79a45dbc5e88df115}
      \strng{authorfullhash}{750cc371a8ded9f79a45dbc5e88df115}
      \field{sortinit}{3}
      \field{sortinithash}{a4b52e5432884761f50fb9571273b93e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0165-5515}
      \field{journaltitle}{Journal of Information Science}
      \field{month}{2}
      \field{number}{1}
      \field{title}{{The automatic identification of stop words}}
      \field{volume}{18}
      \field{year}{1992}
      \field{pages}{45\bibrangedash 55}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1177/016555159201800106
      \endverb
      \verb{urlraw}
      \verb http://journals.sagepub.com/doi/10.1177/016555159201800106
      \endverb
      \verb{url}
      \verb http://journals.sagepub.com/doi/10.1177/016555159201800106
      \endverb
    \endentry
    \entry{Bobadilla2013}{article}{}
      \name{author}{4}{}{%
        {{hash=462d00982581b220dea45c0d7aab8e05}{%
           family={Bobadilla},
           familyi={B\bibinitperiod},
           given={J.},
           giveni={J\bibinitperiod}}}%
        {{hash=ce5b986828050185022d3471acd5f188}{%
           family={Ortega},
           familyi={O\bibinitperiod},
           given={F.},
           giveni={F\bibinitperiod}}}%
        {{hash=04fea2a59c8edf8cec04cfbb9f4345d9}{%
           family={Hernando},
           familyi={H\bibinitperiod},
           given={A.},
           giveni={A\bibinitperiod}}}%
        {{hash=97e4577ea53439118bfb74d37d9e6ade}{%
           family={Guti{é}rrez},
           familyi={G\bibinitperiod},
           given={A.},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Elsevier B.V.}%
      }
      \strng{namehash}{2210aa869758f6c98ff5a0bf9aa89c3f}
      \strng{fullhash}{d8f984d52e7b9b075075979392a862af}
      \strng{bibnamehash}{2210aa869758f6c98ff5a0bf9aa89c3f}
      \strng{authorbibnamehash}{2210aa869758f6c98ff5a0bf9aa89c3f}
      \strng{authornamehash}{2210aa869758f6c98ff5a0bf9aa89c3f}
      \strng{authorfullhash}{d8f984d52e7b9b075075979392a862af}
      \field{sortinit}{4}
      \field{sortinithash}{11cdaee3b18e01d77f3f428b13c1fc76}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recommender systems have developed in parallel with the web. They were initially based on demographic, content-based and collaborative filtering. Currently, these systems are incorporating social information. In the future, they will use implicit, local and personal information from the Internet of things. This article provides an overview of recommender systems as well as collaborative filtering methods and algorithms; it also explains their evolution, provides an original classification for these systems, identifies areas of future implementation and develops certain areas selected for past, present or future importance. {©} 2013 Elsevier B.V. All rights reserved.}
      \field{eprinttype}{arXiv}
      \field{isbn}{0950-7051}
      \field{issn}{09507051}
      \field{journaltitle}{Knowledge-Based Systems}
      \field{title}{{Recommender systems survey}}
      \field{volume}{46}
      \field{year}{2013}
      \field{pages}{109\bibrangedash 132}
      \range{pages}{24}
      \verb{doi}
      \verb 10.1016/j.knosys.2013.03.012
      \endverb
      \verb{eprint}
      \verb arXiv:1011.1669v3
      \endverb
      \verb{file}
      \verb :E$\backslash$:/ACADEMICS/PEC---/ME 4 SEM/mendeley/2013/Recommender systems survey - Bobadilla et al.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://dx.doi.org/10.1016/j.knosys.2013.03.012
      \endverb
      \verb{url}
      \verb http://dx.doi.org/10.1016/j.knosys.2013.03.012
      \endverb
      \keyw{Cold-start,Collaborative filtering,Evaluation metrics,Hybrid,Internet of things,Prediction,Recommendation,Recommender systems,Similarity measures,Social}
    \endentry
    \entry{Salakhutdinov2007}{article}{}
      \name{author}{2}{}{%
        {{hash=ff4643153f1f765d12f548c1fa292974}{%
           family={Salakhutdinov},
           familyi={S\bibinitperiod},
           given={R},
           giveni={R\bibinitperiod}}}%
        {{hash=0069c160921e89ca0f8c83c157dd0e8c}{%
           family={Mnih},
           familyi={M\bibinitperiod},
           given={A},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{5c2fd6e6b0ff1a91f7d179ec8bab4c3e}
      \strng{fullhash}{5c2fd6e6b0ff1a91f7d179ec8bab4c3e}
      \strng{bibnamehash}{5c2fd6e6b0ff1a91f7d179ec8bab4c3e}
      \strng{authorbibnamehash}{5c2fd6e6b0ff1a91f7d179ec8bab4c3e}
      \strng{authornamehash}{5c2fd6e6b0ff1a91f7d179ec8bab4c3e}
      \strng{authorfullhash}{5c2fd6e6b0ff1a91f7d179ec8bab4c3e}
      \field{sortinit}{5}
      \field{sortinithash}{3c19c3776b658b3558e9e2e4840c01e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Many existing approaches to collaborative filtering can neither handle very large datasets nor easily deal with users who have very few ratings. In this paper we present the ProbabilisticMatrix Factorization (PMF) model which scales linearly with the number of observations and, more importantly, performs well on the large, sparse, and very imbalanced Netflix dataset. We further extend the PMF model to include an adaptive prior on the model parameters and show how the model capacity can be controlled automatically. Finally, we introduce a con- strained version of the PMF model that is based on the assumption that users who have rated similar sets ofmovies are likely to have similar preferences. The result- ingmodel is able to generalize considerably better for userswith very few ratings. When the predictions of multiple PMF models are linearly combined with the predictions of Restricted Boltzmann Machines models, we achieve an error rate of 0.8861, that is nearly 7{\%} better than the score of Netflix's own system. 1}
      \field{eprinttype}{arXiv}
      \field{isbn}{9781605582054}
      \field{issn}{1049-5258}
      \field{journaltitle}{Proc. Advances in Neural Information Processing Systems 20 (NIPS 07)}
      \field{title}{{Probabilistic Matrix Factorization.}}
      \field{year}{2007}
      \field{pages}{1257\bibrangedash 1264}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1145/1390156.1390267
      \endverb
      \verb{eprint}
      \verb 1705.05355
      \endverb
      \verb{file}
      \verb :E$\backslash$:/ACADEMICS/PEC---/ME 4 SEM/mendeley/2007/Probabilistic Matrix Factorization. - Salakhutdinov, Mnih.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://discovery.ucl.ac.uk/63248/
      \endverb
      \verb{url}
      \verb http://discovery.ucl.ac.uk/63248/
      \endverb
    \endentry
    \entry{Liu2003}{article}{}
      \name{author}{4}{}{%
        {{hash=0c12ae96450bf90f74282991aa068345}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={T},
           giveni={T\bibinitperiod}}}%
        {{hash=92faacc6874983183ce7fb86e39f31ec}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={S},
           giveni={S\bibinitperiod}}}%
        {{hash=05694d0c858ee219eae7bb764b27b7f6}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Zheng},
           giveni={Z\bibinitperiod}}}%
        {{hash=d02adcb5b8ef866e4ed911e25e715786}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Wy},
           giveni={W\bibinitperiod}}}%
      }
      \strng{namehash}{826e24c781879eb1bc2b587aa8839b98}
      \strng{fullhash}{dd57f19b413d917f190ba4ed5421f4d6}
      \strng{bibnamehash}{826e24c781879eb1bc2b587aa8839b98}
      \strng{authorbibnamehash}{826e24c781879eb1bc2b587aa8839b98}
      \strng{authornamehash}{826e24c781879eb1bc2b587aa8839b98}
      \strng{authorfullhash}{dd57f19b413d917f190ba4ed5421f4d6}
      \field{sortinit}{6}
      \field{sortinithash}{57e57fb8451e7fcfa45d1e069f6d3136}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Feature selection methods have been successfully applied to text categorization but seldom applied to text clustering due to the unavailability of class label information. In this paper, we first give empirical evidence that feature selection methods can improve the efficiency and performance of text clustering algorithm. Then we propose a new feature selection method called “Term Contribution (TC)” and perform a comparative study on a variety of feature selection methods for text clustering, including Document Frequency (DF), Term Strength (TS), Entropy-based (En), Information Gain (IG) and 2 statistic (CHI). Finally, we propose an “Iterative Feature Selection (IF)” method that addresses the unavailability of label problem by utilizing effective supervised feature selection method to iteratively select features and perform clustering. Detailed experimental results on Web Directory data are provided in the paper.}
      \field{eprinttype}{arXiv}
      \field{isbn}{1577351894}
      \field{issn}{9792602550}
      \field{journaltitle}{Icml}
      \field{title}{{An evaluation on feature selection for text clustering}}
      \field{year}{2003}
      \field{pages}{488\bibrangedash 495}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1017/CBO9781107415324.004
      \endverb
      \verb{eprint}
      \verb arXiv:1011.1669v3
      \endverb
      \verb{file}
      \verb :E$\backslash$:/ACADEMICS/PEC---/ME 4 SEM/mendeley/2003/An evaluation on feature selection for text clustering - Liu et al.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://www.aaai.org/Papers/ICML/2003/ICML03-065.pdf
      \endverb
      \verb{url}
      \verb http://www.aaai.org/Papers/ICML/2003/ICML03-065.pdf
      \endverb
    \endentry
    \entry{Koren2010}{article}{}
      \name{author}{1}{}{%
        {{hash=fce0a5d2aa4ba307f448e452e68fdbcb}{%
           family={Koren},
           familyi={K\bibinitperiod},
           given={Yehuda},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{fce0a5d2aa4ba307f448e452e68fdbcb}
      \strng{fullhash}{fce0a5d2aa4ba307f448e452e68fdbcb}
      \strng{bibnamehash}{fce0a5d2aa4ba307f448e452e68fdbcb}
      \strng{authorbibnamehash}{fce0a5d2aa4ba307f448e452e68fdbcb}
      \strng{authornamehash}{fce0a5d2aa4ba307f448e452e68fdbcb}
      \strng{authorfullhash}{fce0a5d2aa4ba307f448e452e68fdbcb}
      \field{sortinit}{7}
      \field{sortinithash}{c818dd9105a2852444fc9f5e145c294e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recommender systems provide users with personalized suggestions for products or services. These systems often rely on collaborating filtering (CF), where past transactions are analyzed in order to establish connections between users and products. The most common approach to CF is based on neighborhood models, which originate from similarities between products or users. In this work we introduce a new neighborhood model with an improved prediction accuracy. Unlike previous approaches that are based on heuristic similarities,we model neighborhood relations by minimizing a global cost function. Further accuracy improvements are achieved by extending the model to exploit both explicit and implicit feedback by the users. Past models were limited by the need to compute all pairwise similarities between items or users, which grow quadratically with input size. In particular, this limitation vastly complicates adopting user similarity models, due to the typical large number of users. Our new model solves these limitations by factoring the neighborhood model, thus making both item-item and user-user implementations scale linearly with the size of the data. The methods are tested on the Netflix data, with encouraging results. {©} 2010 ACM.}
      \field{eprinttype}{arXiv}
      \field{isbn}{0027-8424}
      \field{issn}{15564681}
      \field{journaltitle}{ACM Transactions on Knowledge Discovery from Data}
      \field{number}{1}
      \field{title}{{Factor in the neighbors}}
      \field{volume}{4}
      \field{year}{2010}
      \field{pages}{1\bibrangedash 24}
      \range{pages}{24}
      \verb{doi}
      \verb 10.1145/1644873.1644874
      \endverb
      \verb{eprint}
      \verb 1608.05878
      \endverb
      \verb{file}
      \verb :E$\backslash$:/ACADEMICS/PEC---/ME 4 SEM/mendeley/2010/Factor in the neighbors - Koren.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://portal.acm.org/citation.cfm?doid=1644873.1644874
      \endverb
      \verb{url}
      \verb http://portal.acm.org/citation.cfm?doid=1644873.1644874
      \endverb
    \endentry
    \entry{Koren2008}{article}{}
      \name{author}{1}{}{%
        {{hash=fce0a5d2aa4ba307f448e452e68fdbcb}{%
           family={Koren},
           familyi={K\bibinitperiod},
           given={Yehuda},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{fce0a5d2aa4ba307f448e452e68fdbcb}
      \strng{fullhash}{fce0a5d2aa4ba307f448e452e68fdbcb}
      \strng{bibnamehash}{fce0a5d2aa4ba307f448e452e68fdbcb}
      \strng{authorbibnamehash}{fce0a5d2aa4ba307f448e452e68fdbcb}
      \strng{authornamehash}{fce0a5d2aa4ba307f448e452e68fdbcb}
      \strng{authorfullhash}{fce0a5d2aa4ba307f448e452e68fdbcb}
      \field{sortinit}{8}
      \field{sortinithash}{07edf88d4ea82509b9c4b4d13f41c452}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recommender systems provide users with personalized suggestions for products or services. These systems often rely on Collaborating Filtering (CF), where past transactions are analyzed in order to establish connections between users and products. The two more successful approaches to CF are latent factor models, which directly profile both users and products, and neighborhood models, which analyze similarities between products or users. In this work we introduce some innovations to both approaches. The factor and neighborhood models can now be smoothly merged, thereby building a more accurate combined model. Further accuracy improvements are achieved by extending the models to exploit both explicit and implicit feedback by the users. The methods are tested on the Netflix data. Results are better than those previously published on that dataset. In addition, we suggest a new evaluation metric, which highlights the differences among methods, based on their performance at a top-K recommendation task.}
      \field{eprinttype}{arXiv}
      \field{isbn}{9781605581934}
      \field{issn}{1605581933}
      \field{journaltitle}{Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD 08}
      \field{title}{{Factorization meets the neighborhood}}
      \field{year}{2008}
      \field{pages}{426}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1145/1401890.1401944
      \endverb
      \verb{eprint}
      \verb 62
      \endverb
      \verb{file}
      \verb :E$\backslash$:/ACADEMICS/PEC---/ME 4 SEM/mendeley/2008/Factorization meets the neighborhood - Koren.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://dl.acm.org/citation.cfm?doid=1401890.1401944
      \endverb
      \verb{url}
      \verb http://dl.acm.org/citation.cfm?doid=1401890.1401944
      \endverb
      \keyw{collaborative filtering,recommender systems}
    \endentry
  \enddatalist
\endrefsection
\endinput

