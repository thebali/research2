% $ biblatex auxiliary file $
% $ biblatex bbl format version 2.9 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{none/global//global/global}
    \entry{Park2012}{article}{}
      \name{author}{4}{}{%
        {{hash=5b745df0961f7e29675185b7e9258a44}{%
           family={Park},
           familyi={P\bibinitperiod},
           given={Deuk\bibnamedelima Hee},
           giveni={D\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=4e296c0f6c7f55ce8f119caa77ac5636}{%
           family={Kim},
           familyi={K\bibinitperiod},
           given={Hyea\bibnamedelima Kyeong},
           giveni={H\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=2168c8f0e63b7f61dc342105016ff52f}{%
           family={Choi},
           familyi={C\bibinitperiod},
           given={Il\bibnamedelima Young},
           giveni={I\bibinitperiod\bibinitdelim Y\bibinitperiod}}}%
        {{hash=ed47f99ac8789a656edb6c8e7686d7a1}{%
           family={Kim},
           familyi={K\bibinitperiod},
           given={Jae\bibnamedelima Kyeong},
           giveni={J\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Elsevier Ltd}%
      }
      \strng{namehash}{5ff214b4297f096d8b8d44b3a25d1d87}
      \strng{fullhash}{e68648e54839c0c44bbb9887351232d2}
      \strng{bibnamehash}{5ff214b4297f096d8b8d44b3a25d1d87}
      \strng{authorbibnamehash}{5ff214b4297f096d8b8d44b3a25d1d87}
      \strng{authornamehash}{5ff214b4297f096d8b8d44b3a25d1d87}
      \strng{authorfullhash}{e68648e54839c0c44bbb9887351232d2}
      \field{sortinit}{1}
      \field{sortinithash}{2174f786c6195e7fe2ee1c229b416e29}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recommender systems have become an important research field since the emergence of the first paper on collaborative filtering in the mid-1990s. Although academic research on recommender systems has increased significantly over the past 10 years, there are deficiencies in the comprehensive literature review and classification of that research. For that reason, we reviewed 210 articles on recommender systems from 46 journals published between 2001 and 2010, and then classified those by the year of publication, the journals in which they appeared, their application fields, and their data mining techniques. The 210 articles are categorized into eight application fields (books, documents, images, movie, music, shopping, TV programs, and others) and eight data mining techniques (association rule, clustering, decision tree, k-nearest neighbor, link analysis, neural network, regression, and other heuristic methods). Our research provides information about trends in recommender systems research by examining the publication years of the articles, and provides practitioners and researchers with insight and future direction on recommender systems. We hope that this paper helps anyone who is interested in recommender systems research with insight for future research direction. {©} 2012 Elsevier Ltd. All rights reserved.}
      \field{isbn}{09574174}
      \field{issn}{09574174}
      \field{journaltitle}{Expert Systems with Applications}
      \field{number}{11}
      \field{title}{{A literature review and classification of recommender systems research}}
      \field{volume}{39}
      \field{year}{2012}
      \field{pages}{10059\bibrangedash 10072}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1016/j.eswa.2012.02.038
      \endverb
      \verb{urlraw}
      \verb http://dx.doi.org/10.1016/j.eswa.2012.02.038
      \endverb
      \verb{url}
      \verb http://dx.doi.org/10.1016/j.eswa.2012.02.038
      \endverb
      \keyw{Classification,Data mining technique,Literature review,Recommender systems}
    \endentry
    \entry{Wilbur1992}{article}{}
      \name{author}{2}{}{%
        {{hash=37556444561be148ae5a79fb2c40f338}{%
           family={Wilbur},
           familyi={W\bibinitperiod},
           given={W.\bibnamedelimi John},
           giveni={W\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=a4e468755ab07dede54e21de210b128b}{%
           family={Sirotkin},
           familyi={S\bibinitperiod},
           given={Karl},
           giveni={K\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Sage Publications, Inc.}%
      }
      \strng{namehash}{750cc371a8ded9f79a45dbc5e88df115}
      \strng{fullhash}{750cc371a8ded9f79a45dbc5e88df115}
      \strng{bibnamehash}{750cc371a8ded9f79a45dbc5e88df115}
      \strng{authorbibnamehash}{750cc371a8ded9f79a45dbc5e88df115}
      \strng{authornamehash}{750cc371a8ded9f79a45dbc5e88df115}
      \strng{authorfullhash}{750cc371a8ded9f79a45dbc5e88df115}
      \field{sortinit}{3}
      \field{sortinithash}{a4b52e5432884761f50fb9571273b93e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0165-5515}
      \field{journaltitle}{Journal of Information Science}
      \field{month}{2}
      \field{number}{1}
      \field{title}{{The automatic identification of stop words}}
      \field{volume}{18}
      \field{year}{1992}
      \field{pages}{45\bibrangedash 55}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1177/016555159201800106
      \endverb
      \verb{urlraw}
      \verb http://journals.sagepub.com/doi/10.1177/016555159201800106
      \endverb
      \verb{url}
      \verb http://journals.sagepub.com/doi/10.1177/016555159201800106
      \endverb
    \endentry
    \entry{Bobadilla2013}{article}{}
      \name{author}{4}{}{%
        {{hash=462d00982581b220dea45c0d7aab8e05}{%
           family={Bobadilla},
           familyi={B\bibinitperiod},
           given={J.},
           giveni={J\bibinitperiod}}}%
        {{hash=ce5b986828050185022d3471acd5f188}{%
           family={Ortega},
           familyi={O\bibinitperiod},
           given={F.},
           giveni={F\bibinitperiod}}}%
        {{hash=04fea2a59c8edf8cec04cfbb9f4345d9}{%
           family={Hernando},
           familyi={H\bibinitperiod},
           given={A.},
           giveni={A\bibinitperiod}}}%
        {{hash=97e4577ea53439118bfb74d37d9e6ade}{%
           family={Guti{é}rrez},
           familyi={G\bibinitperiod},
           given={A.},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Elsevier B.V.}%
      }
      \strng{namehash}{2210aa869758f6c98ff5a0bf9aa89c3f}
      \strng{fullhash}{d8f984d52e7b9b075075979392a862af}
      \strng{bibnamehash}{2210aa869758f6c98ff5a0bf9aa89c3f}
      \strng{authorbibnamehash}{2210aa869758f6c98ff5a0bf9aa89c3f}
      \strng{authornamehash}{2210aa869758f6c98ff5a0bf9aa89c3f}
      \strng{authorfullhash}{d8f984d52e7b9b075075979392a862af}
      \field{sortinit}{4}
      \field{sortinithash}{11cdaee3b18e01d77f3f428b13c1fc76}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recommender systems have developed in parallel with the web. They were initially based on demographic, content-based and collaborative filtering. Currently, these systems are incorporating social information. In the future, they will use implicit, local and personal information from the Internet of things. This article provides an overview of recommender systems as well as collaborative filtering methods and algorithms; it also explains their evolution, provides an original classification for these systems, identifies areas of future implementation and develops certain areas selected for past, present or future importance. {©} 2013 Elsevier B.V. All rights reserved.}
      \field{eprinttype}{arXiv}
      \field{isbn}{0950-7051}
      \field{issn}{09507051}
      \field{journaltitle}{Knowledge-Based Systems}
      \field{title}{{Recommender systems survey}}
      \field{volume}{46}
      \field{year}{2013}
      \field{pages}{109\bibrangedash 132}
      \range{pages}{24}
      \verb{doi}
      \verb 10.1016/j.knosys.2013.03.012
      \endverb
      \verb{eprint}
      \verb arXiv:1011.1669v3
      \endverb
      \verb{file}
      \verb :E$\backslash$:/ACADEMICS/PEC---/ME 4 SEM/mendeley/2013/Recommender systems survey - Bobadilla et al.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://dx.doi.org/10.1016/j.knosys.2013.03.012
      \endverb
      \verb{url}
      \verb http://dx.doi.org/10.1016/j.knosys.2013.03.012
      \endverb
      \keyw{Cold-start,Collaborative filtering,Evaluation metrics,Hybrid,Internet of things,Prediction,Recommendation,Recommender systems,Similarity measures,Social}
    \endentry
    \entry{Salakhutdinov2007}{article}{}
      \name{author}{2}{}{%
        {{hash=ff4643153f1f765d12f548c1fa292974}{%
           family={Salakhutdinov},
           familyi={S\bibinitperiod},
           given={R},
           giveni={R\bibinitperiod}}}%
        {{hash=0069c160921e89ca0f8c83c157dd0e8c}{%
           family={Mnih},
           familyi={M\bibinitperiod},
           given={A},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{5c2fd6e6b0ff1a91f7d179ec8bab4c3e}
      \strng{fullhash}{5c2fd6e6b0ff1a91f7d179ec8bab4c3e}
      \strng{bibnamehash}{5c2fd6e6b0ff1a91f7d179ec8bab4c3e}
      \strng{authorbibnamehash}{5c2fd6e6b0ff1a91f7d179ec8bab4c3e}
      \strng{authornamehash}{5c2fd6e6b0ff1a91f7d179ec8bab4c3e}
      \strng{authorfullhash}{5c2fd6e6b0ff1a91f7d179ec8bab4c3e}
      \field{sortinit}{5}
      \field{sortinithash}{3c19c3776b658b3558e9e2e4840c01e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Many existing approaches to collaborative filtering can neither handle very large datasets nor easily deal with users who have very few ratings. In this paper we present the ProbabilisticMatrix Factorization (PMF) model which scales linearly with the number of observations and, more importantly, performs well on the large, sparse, and very imbalanced Netflix dataset. We further extend the PMF model to include an adaptive prior on the model parameters and show how the model capacity can be controlled automatically. Finally, we introduce a con- strained version of the PMF model that is based on the assumption that users who have rated similar sets ofmovies are likely to have similar preferences. The result- ingmodel is able to generalize considerably better for userswith very few ratings. When the predictions of multiple PMF models are linearly combined with the predictions of Restricted Boltzmann Machines models, we achieve an error rate of 0.8861, that is nearly 7{\%} better than the score of Netflix's own system. 1}
      \field{eprinttype}{arXiv}
      \field{isbn}{9781605582054}
      \field{issn}{1049-5258}
      \field{journaltitle}{Proc. Advances in Neural Information Processing Systems 20 (NIPS 07)}
      \field{title}{{Probabilistic Matrix Factorization.}}
      \field{year}{2007}
      \field{pages}{1257\bibrangedash 1264}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1145/1390156.1390267
      \endverb
      \verb{eprint}
      \verb 1705.05355
      \endverb
      \verb{file}
      \verb :E$\backslash$:/ACADEMICS/PEC---/ME 4 SEM/mendeley/2007/Probabilistic Matrix Factorization. - Salakhutdinov, Mnih.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://discovery.ucl.ac.uk/63248/
      \endverb
      \verb{url}
      \verb http://discovery.ucl.ac.uk/63248/
      \endverb
    \endentry
    \entry{Liu2003}{article}{}
      \name{author}{4}{}{%
        {{hash=0c12ae96450bf90f74282991aa068345}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={T},
           giveni={T\bibinitperiod}}}%
        {{hash=92faacc6874983183ce7fb86e39f31ec}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={S},
           giveni={S\bibinitperiod}}}%
        {{hash=05694d0c858ee219eae7bb764b27b7f6}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Zheng},
           giveni={Z\bibinitperiod}}}%
        {{hash=d02adcb5b8ef866e4ed911e25e715786}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Wy},
           giveni={W\bibinitperiod}}}%
      }
      \strng{namehash}{826e24c781879eb1bc2b587aa8839b98}
      \strng{fullhash}{dd57f19b413d917f190ba4ed5421f4d6}
      \strng{bibnamehash}{826e24c781879eb1bc2b587aa8839b98}
      \strng{authorbibnamehash}{826e24c781879eb1bc2b587aa8839b98}
      \strng{authornamehash}{826e24c781879eb1bc2b587aa8839b98}
      \strng{authorfullhash}{dd57f19b413d917f190ba4ed5421f4d6}
      \field{sortinit}{6}
      \field{sortinithash}{57e57fb8451e7fcfa45d1e069f6d3136}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Feature selection methods have been successfully applied to text categorization but seldom applied to text clustering due to the unavailability of class label information. In this paper, we first give empirical evidence that feature selection methods can improve the efficiency and performance of text clustering algorithm. Then we propose a new feature selection method called “Term Contribution (TC)” and perform a comparative study on a variety of feature selection methods for text clustering, including Document Frequency (DF), Term Strength (TS), Entropy-based (En), Information Gain (IG) and 2 statistic (CHI). Finally, we propose an “Iterative Feature Selection (IF)” method that addresses the unavailability of label problem by utilizing effective supervised feature selection method to iteratively select features and perform clustering. Detailed experimental results on Web Directory data are provided in the paper.}
      \field{eprinttype}{arXiv}
      \field{isbn}{1577351894}
      \field{issn}{9792602550}
      \field{journaltitle}{Icml}
      \field{title}{{An evaluation on feature selection for text clustering}}
      \field{year}{2003}
      \field{pages}{488\bibrangedash 495}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1017/CBO9781107415324.004
      \endverb
      \verb{eprint}
      \verb arXiv:1011.1669v3
      \endverb
      \verb{file}
      \verb :E$\backslash$:/ACADEMICS/PEC---/ME 4 SEM/mendeley/2003/An evaluation on feature selection for text clustering - Liu et al.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://www.aaai.org/Papers/ICML/2003/ICML03-065.pdf
      \endverb
      \verb{url}
      \verb http://www.aaai.org/Papers/ICML/2003/ICML03-065.pdf
      \endverb
    \endentry
  \enddatalist
\endrefsection
\endinput

