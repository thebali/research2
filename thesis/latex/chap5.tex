\chapter{Methodology}
The area for the system is the.

SVD



-----5CHAPTER 5. IMPLEMENTATION

Implementation

The implementation consists in a full scale implementation of the Contextual
Multi-armed bandit algorithm using Thompson Sampling, data extraction,
construction of context vectors as well as algorithms for evaluation. Because of
each implementation of a Multi-Armed Bandit algorithm being unique depending on the actual use-area and dataset, this chapter will focus on an implementation for ecommerce, and more specifically the Junkyard dataset. The implemented recommender
system is built exclusively with Multi-Armed Bandits with its associated techniques
having a user experience-based approach rather than a money-making one. The repository for the full code-base can be found at https://github.com/FredrikEk/MasterThesis-2015-Contextual-Multi-Armed-Bandits.

T
5.1

Approach

As can be seen in chapter 2 the diversity of recommender systems with methods and
algorithms for implementing them are huge and rather complex. The implementation
performed in this thesis takes a user-centric approach taking most influences from a
content-based filtering approach. Due to the algorithmic and machine-learning nature
of bandit algorithms, thinking in the ways of content-based filtering when applying
methods of Contextual Multi-Armed bandit algorithms comes naturally.

5.2

Thompson Sampling

The implementation of Multi Armed Bandits in this thesis uses Thompson Sampling
as the arm learning-policy. It helps the algorithm to update which arm being the best
at a specific point in time on-the-fly while being able to predict different recommendations even without any offline calculations. In contrary to the UCB method, Thompson
Sampling generate items randomly, which means that it could have an advantage in
23

5.3. ALGORITHM


e-commerce systems where it would be bad if the same set of items was returned repeatedly until a recalculation is made. It is however an open question, whether this is the
case or not, that needs to be evaluated with fixed static data, as is the case for this thesis. Something that could be done using synthetic data modelling (7.2.1), but that have
not been done in this thesis. The UCB method requires an offline recalculation before
new items are recommended. Why this is bad is extra obvious in a setting where a web
page might get updated every time a user navigates to a new view, as using Thompson
Sampling, would mean generating different items each time a page is updated/reloaded
and the algorithm thereby has a higher chance of predicting something that the user
finds interesting and perhaps even wants to buy.

5.3

Algorithm

For each purchase (session) the algorithm initially draws a random sample θ from a beta
distribution, using each arm’s α- and β- variables. The arm arm with highest θ will
be the arm used to perform the prediction for this session. In order to find the top
ten items, considered the best by arm, the algorithm uses the contextual vector of the
current user user together with all the items in the dataset. The contextual vector of
user is element-wise multiplied with the contextual vector of an item item, which in
turn is scalar multiplied with the contextual vector of arm. The resulting value is a
measurement of how good item is for user according to arm. The ten items with the
highest measurement values then gets recommended. If any of these items are bought
by user, the prediction of arm is considered correct and the α parameter of arm is
increased by one. On the other hand if the recommended items turn out to be faulty

24

5.4. DATA EXTRACTION

CHAPTER 5. IMPLEMENTATION

predictions the β is increased by one, whereas the α is left unchanged.
Algorithm 5: Multi-armed bandit algorithm
Data: A: the arms, P: the purchases, I: all the items, U: all the users
1 forall the p in P do
2
forall the a in A do
3
θa ← BetaDistribution(αa , βa )
4
end
5
arm ← arg max (θa )
6
user ← puser
7
forall the item in I do
8
itemreward = armcontext · (usercontext ∗ itemcontext )
9
end
10
ListOfItems ← top10 arg max (itemreward )
11
itemsBought ← pitems
12
if itemsBought contains ListOfItems then
13
reward ← 1
14
else
15
reward ← 0
16
end
17
updateArm(arm, reward) // Update the arm depending on success or failure
18
updateItems(itemsBought, user) // Update the items which were bought
19
updateU ser(user, itemsBought) // Update the user who bought items
20 end
For more information on how the context vectors, armcontext , usercontext , itemcontext )
work and are built up see section 5.5.

5.3.1

Choosing the number of arms

In the implemented application of this thesis, the algorithm use 50 arms. A good unofficial measurement that has been found experimentally in this project in combination
with studies of current literature is that the number of arms should exceed the total
number of feature-elements in the arm context vector. It was noticed that using too few
arms could mean not generating good enough variance of the elements, while using too
many arms would hurt the performance, in terms of successful recommendations, when
using small datasets.

5.4

Data extraction

The data used in the implementation is extracted from an MSSQL database containing
the entire Junkyard dataset. The extracted data that are used for training, recommendations and validation consists of three SQL- views. Each containing context of users,
items and orders respectively.

25

5.5. CONTEXT VECTORS

5.4.1

CHAPTER 5. IMPLEMENTATION

The Userdata view

The query that builds the userdata view can be viewed in the appendix, listing A.1. The
columns UserId, ZipCode, DateOfBirth and Gender in the resulting table are used when
constructing the context vector of each user (see more under 5.5.1) whereas the column
Updated is used to check when the specific user last updated his profile. Most of the
time it is used for notifying when a user changed it’s domicile i.e the ZipCode changed.
Something that is important because one of the features of a user’s context vector is
domicile.

5.4.2

The Itemdata view

The query that builds the itemdata view can be viewed in the appendix, listing A.2. The
columns ItemId, Gender and category in the resulting table are used when constructing
the context vector of each item (see more under 5.5.2) where as the columns ItemCreated,
ItemModified and CurrentStock are used to check the saldo of an item i.e if the saldo is
at zero at a specific point in time, when a recommendation is about to be made and we
know that the saldo wont increase in the near future, then obviously we do not want to
recommend that item as it would contribute to a bad user experience.

5.4.3

The Orderdata view

The query that builds the orderdata view can be viewed in the appendix, listing A.3. All
of the columns are used to train the arms of the bandit algorithm, from one point in time
to another. This view is also, however, used for validation. By chronologically observing
the data, ”future” data can be used as validation. Simplified, the algorithm looks at
which user bought what item and at what point in time. More about the training part
of the algorithm can be found under 5.6.

5.5

Context vectors

Each user, item and arm has it’s own context vector. The vectors of each arm are generated randomly, whereas the Thompson Sampling algorithm determines which arm is the
best, currently. The user and item vectors are constructed using information provided
by users upon registering, by their history of purchases and from their behaviour in the
system, whereas itemdata gets added when administrators of the system add new items
to the database.
In the following sections the context vectors are explained in more detail. The number of elements each feature is represented by are shown in Table 5.1.

26

5.5. CONTEXT VECTORS

CHAPTER 5. IMPLEMENTATION

Feature

Number of Elements

Gender

2

Category

33

Age

3

ZipCode

1

Popularity

2

Total

41

Table 5.1: Feature Data

The representation of Gender is done using one element each for male and female respectively. The Category feature has one element for each category. Age is represented
by three age spans; young − the buyer is younger than 18 years old, middle aged −
the buyer is between 18 and 33, old − where the buyer is older than 33. ZipCode is
represented by only one element consisting of the three first digits in the area-number.
The Popularity feature contains two elements; most bought last week and most bought
last month.

5.5.1

The User Context Vector

The implementation of the user context vectors follow this simple model:
Gender = Given by the user/database, can be male or female
Category = By observing the category of purchases from a specific user
Age = Given by the user/database
ZipCode = Given by the user/database
Popularity = By observing if the user buys popular items or not

5.5.2

The Item Context Vector

The implementation of the item context vectors follow this simple model:
Gender = Given by the admin/database, can be male, female or both
Category = Given by the admin/database
Age = By observing the age span of users who bought this item
ZipCode = By observing the domicile of users who bought this item
Popularity = By looking at the most bought items from the last week and the last month

27

5.6. TRAINING ELEMENTS

5.5.3

CHAPTER 5. IMPLEMENTATION

The Arm Context Vector

The purpose of the context vectors of the arms is to control how much weight to put
on the different features in the context vectors of items and users when computing their
scalar product. In the initialisation phase of the algorithm, the feature-values of the
arm’s context vectors are generated randomly from a continuously uniform distribution,
to values between 0 and 1. After which they never change.

Gender = Randomly
Category = Randomly
Age = Randomly
ZipCode = Randomly
Popularity = Randomly

generated
generated
generated
generated
generated

These vectors are needed in the Contextual Multi-Armed Bandit setting as described in
3.4.

5.6

Training elements

As mentioned in 5.1 the implemented set of algorithms are based on a content-based
filtering approach, using learning techniques for ”arms”, items and users in the setting of
Multi-Armed Bandits. The method used for the training of arms are called Thompson
Sampling and can be viewed under 5.2. The training of user profiles and items follow a
user preference model [60]. The training consists in updating the features of the context
vectors.

5.6.1

Training of Arms

The context vectors of the arms are initialised with random values as described under
5.5.3 and training the arms only consists of making sure we use the better arms more
often.

5.6.2

Training of User profiles

Assuming that a specific user is likely to buy products with equal features in the future,
user profiles progress over time and learn the features of items that are bought by the
specific user. Item features contributing to this learning process are category, gender and
popularity. The structure of the user context vector are shown in 5.5.1. User profiles get
initial training prior to running the algorithm.

28

--------5.7. FRAMEWORKS

5.6.3

CHAPTER 5. IMPLEMENTATION

Training of Items

Items learn the preferences of the users who purchase them over time, as shown in
5.5.2. As several of the item features rely on classifications made through learning userpreferences, items get initial training prior to running the main algorithm.

5.7

Frameworks for implementing recommender systems

As initially described in Chapter 1 this project uses frameworks for implementing recommender systems. Both of the used frameworks come with a well-documented java API,
which make them really intuitive and user-friendly. The plan, initially, was to use frameworks for redundant efforts such as implementing data structures, basic algorithms and
visual tools etc. This have been true to some extent, but in most cases implementations
have been made from scratch.

5.7.1

Apache Mahout

Apache Mahout is a scalable machine learning library and framework for implementing
recommender systems [2]. The core of the implemented application in this thesis is
written from scratch using data structures and some algorithms from Mahout; Through
use of Mahout, vectors and matrices with their algebraic operations could be used of the
shelf. Mahout also contained some built-in basic machine-learning libraries such as one
for beta distributions, used in this project.

5.7.2

LensKit

LensKit is, like Mahout, a framework for implementing recommender systems with support for some of the more common algorithms[1]. Similarly with Mahout, the implementation of this thesis only make use of some data structures from LensKit.

5.8

Implementation of Collaborative Filtering for Evaluation

As can be read in Chapter 2, the mainstream approach of implementing recommender
systems [61] to date is through use of collaborative filtering- based techniques, which
is why it makes sense to use one for comparison. The implementation of collaborative
filtering in this project is performed taking a user-based approach, meaning that similarity is computed between users rather than items. More about user-based collaborative
filtering can be found under 2.1.1. The implementation structure look like any other
Collaborative Filtering-based approach, using a similarity computation followed by a
prediction generation.

29

--------5.9. BASELINES

5.8.1

CHAPTER 5. IMPLEMENTATION

Similarity Computation

The standard implementations of collaborative filtering based techniques use ratings as
described in 2.1. All systems do not however make use of ratings. For instance, it is
rather trivial to realise why using ratings would not be feasible for garment-based ecommerce, as is the case for the dataset used in this thesis; the Junkyard e-commerce
platform does not make use of ratings.
Our implementation follows a model described in [62], where similarity of users are
computed using the Jaccard Distance of purchases. The Jaccard Similarity is computed
as:
|u1  u2 |
J(u1 ,u2 ) =
(5.1)
|u1  u2 |
Where, in our implementation, u1 and u2 are vectors each belonging to different users,
whose similarity is to be computed. These vectors contain Boolean values for all items in
the database, determining whether a specific item has been bought by this user or not.
Computing the Jaccard Distance as explained above will return a value 0 < J(u1 ,u2 ) < 1
and can thus be seen as ”How similar is user u1 to user u2 ?”.

5.8.2

Prediction Generation

After the similarity is computed between a user u and all other users u ∈ {u1 , u2 , . . . , un }
who bought items, the algorithm chooses which items to recommend using the following
formulae:
Sik =

X

J(uj , uk )

(5.2)

i∈uj ,j6=k

P (uk ) = top 10 Sik

(5.3)

Where the resulting Sik is the value of item i, where i is an item that user k has not
bought. J is the similarity function for a user uj and a user uk described in equation
5.1. P (uk ) is a vector containing the top ten items for user uk .

5.9

Recommender Baselines for Evaluation

Aside from the implemented, significantly different algorithms used for evaluation described in 5.8, some basic statistical models have been implemented to be used as a
kind of baselines throughout the the project. A baseline is a static algorithm for making recommendations, following some statistical model. Some examples of implemented
baselines are (strategy):
• Always recommend the most sold item, as of the last hours, days, weeks, months
or season. (People always buy the most popular items)

30

5.9. BASELINES

CHAPTER 5. IMPLEMENTATION

• Always recommend items based on gender. (Men and women always buy the same
items as other men and women, respectively)
• Always recommend items based on age. (People at a certain age always buy same
items as other people in the same age)
• Always recommend items based on domicile. (People from city X always buy the
same items as other people from city X)
Baselines have been used before any other extensive evaluation method, something that
has been of great help when implementing the application, as these baselines can be seen
as evaluation in its most basic form.

