%%% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[10pt,conference]{IEEEtran}
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}
%
\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




%\usepackage{array}




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\usepackage[english]{babel}
\usepackage{graphicx, type1cm, lettrine}

% \usepackage[sort&compress, numbers]{natbib}
\usepackage{booktabs}

%% break urls
\usepackage[hyphens]{url}
\usepackage[hidelinks]{hyperref}
\hypersetup{breaklinks=true}
\urlstyle{same}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
% Do not put math or special symbols in the title.
% title actual
% Collaborative filtering: Techniques and Applications
\title{Recommendations based on Matrix Factorization and Latent Dirichlet Allocation}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations

\author{
\IEEEauthorblockN{Rahul Bali}
\IEEEauthorblockA{Department of Computer Science\\
Punjab Engineering College\\
Chandigarh, India\\
Email: rahulrdb18@gmail.com}
% email: rahulbali.mecse16@pec.edu.in
\and
\IEEEauthorblockN{Shilpa Verma}
\IEEEauthorblockA{Department of Computer Science\\
Punjab Engineering College\\
Chandigarh, India\\
Email: shilpaverma.pec@gmail.com}}



% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 

% a kind of multiline comment
\iffalse

\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
Homer Simpson\IEEEauthorrefmark{2},
James Kirk\IEEEauthorrefmark{3}, 
Montgomery Scott\IEEEauthorrefmark{3} and
Eldon Tyrell\IEEEauthorrefmark{4}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
Georgia Institute of Technology,
Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
Email: homer@thesimpsons.com}
\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
Telephone: (800) 555--1212, Fax: (888) 555--1212}
\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}

I don't want this to happen
\fi


% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
	Although latent factor models (e.g., matrix factorization) achieve good accuracy in rating prediction, they suffer from several problems including cold-start, non-transparency, and suboptimal recommendation for local users or items. In this paper, we employ textual review information with ratings to tackle these limitations. In this paper, we propose a unified model that combines content-based filtering with collaborative filtering, harnessing the information of both ratings and reviews. 

	Firstly, we apply a proposed topic model on the review text to model user preferences and item features. We apply topic modeling techniques on the review text and match the topics with rating dimensions to improve prediction accuracy. With the information embedded in the review text, we can alleviate the cold-start problem. Furthermore, our model is able to learn latent topics that are interpretable. With these interpretable topics, we can explore the prior knowledge on items or users and recommend completely "cold" items. 
	Comprehensive experimental studies have been conducted on three datasets from Amazon datasets which show that our proposed model lead to significant improvement compared with strong baseline methods, especially for datasets which are extremely sparse where rating-only methods are not able to make accurate predictions.
\end{abstract}


% no keywords
% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


%% keywords for the writing.
%%% information overload
%%% computer-human interaction
%%% 
\section{Introduction}


Nowadays, most people use the Internet and spend more time on social networks or e-commerce
sites than in the past. The exponential growth of the amount of information on the Internet has
made users face challenges in finding useful information [1 – 3]. Fortunately, studying the users’
behaviors, their preferences can be analyzed. Recommender systems (RS) are used to do so [ 4 ].
Recommender systems adopt to provide recommendations to each user based on their activity,
preferences, and behaviors which are consistent with the users’ personal preferences and assist them
in decision-making [5].


Moreover, MF cannot achieve optimal rating prediction locally for each user-item pair, because it learns the latent factors of users ($p_u$ ) and items ($q_i$) via a global optimization strategy [10]. In other words, $p_u$ and $q_i$ are optimized to achieve a global optimization over all the user-item ratings in the training dataset.1 As a result, the performance could be severely compromised locally for individual users or items. MF predicts an unknown rating by the dot product of the targeted user $u$'s and item $i$'s latent factors. The overall rating of a user towards an item $r_{u,i}$ ) is decided by the importance/contribution of all factors. Take the k-th factor as anexample, its contribution is pu,k ∗qi,k . For accurate prediction, it is important to accurately capture the importance ofeach latent factor for a user towards an item. It is well-known that different users may care about different aspects of an item. For example, in the domain of restaurants, some users care more about the taste of food while others pay more attention to the ambience. Even for the same aspect, the preference of users could be different from each other. For example, in the food aspect, some users like Chinese cuisines while some others favor Italian cuisines. Similarly, the characteristics of items on an aspect could also be different from each other \cite{}.

Recommender systems are implem​ented with three techniques including: content-based [6], knowledge-based [7], and ​collaborative filtering [8]. Collaborative filtering (CF) is one of the most common​ly used techniques in RS [9-11]. In this study, we use collaborative filtering. In CF, opini​ons and ratings of similar users to the target user are used to provide recommendati​ons. The target user is a user who should receive recommendations. In fact, the core ​of CF is to find the similarities between users. CF is possible in two ways: using memo​ry-based and model-based approaches [12]; the combination of these two methods is​ also used [13 , 14]. The memory-based CF is used in this study. In the memory-based ​method, the similarity between users is calculated using one of three techniques of si​milarity algorithms [10, 15], similarity measures [16 – 21], or heuristics methods [22]. Simil​arity measures are used this study. Then, a rating is predicted for the items that are​ not rated by the target user using rating prediction formulas. Finally, the items with​ the highest predicted ratings are recommended to the target user [10,17,23].


\section{Related Work}
When making comments on an ite​m (e.g., product, movie, and restaurant) in the online review/business websites, such​ as Amazon, reviewers also provide an overall rating, which indicates their overall ​preference or satisfaction towards the corresponding items. Hence, predicting users​' overall ratings to unrated items or personalized rating prediction is an import​ant research problem in recommender systems. Latent factor models (e.g., matrix fa​ctorization [9, 21, 37]) are the most widely used and successful techniques for rati​ng prediction, as demonstrated by the Netflix Prize contest [3]. These methods cha​racterize user's interests and item's features using latent factors inferred from r​ating patterns in user-item rating records. As a typical collaborative filtering techni​que, the performance of MF suffers when the ratings of items or users are insufficien​t (also known as the cold-start problem) [17]. Besides, a rating only indicates the ov​erall satisfaction of a user towards an item, it cannot explain the underlying rationa​le. For example, a user could give a restaurant a high rating because of its deliciou​s food or due to its nice ambience. Most existing MF models cannot provide such fin​e-grained analysis. Therefore, relying solely on ratings makes these methods hard to ex​plicitly and accurately model users' preferences [17, 23, 26, 35, 36].

\textbf{Topic-based}: 
In the recent work [16], the a​uthors proposed the Hidden Factors and Hidden Topics (HFT) model, which learnt a La​tent Dirichlet Allocation (LDA) [3] model for items using the review text and a ma​trix factorization model to fit the ratings. To bridge the gap between the stochastic​ vector obtained from LDA and the real- valued vector in MF model, the authors proposed​ a transformation to link the two. Their method demonstrated significant improvement over baseline methods that use ratings or reviews alone. However, the tr​ansformation function they employ, the exponential function, fixed the relationsh​ip between latent vector in MF and the topic distribution. Although a parameter is employed to maintain a more flexible relationship, it is still difficult to ensure that this transformation is correctly scaled.
These approaches extract latent topics or aspects from reviews. An early work [14] in this direction relied on domai​n knowledge to manually label reviews into different aspects, which requires expens​ive domain knowledge and high labor cost. Later on, most works attempt to extract latent topics or aspects from reviews automatically [2, 12, 36, 39]. A​ general approach of these methods is to extract latent topics from reviews using topi​c models [23, 26, 31, 32, 39] or non-negative MF [2, 29] and learn latent factors from ​ratings using MF methods. HFT [26] and TopicMF [2] link the latent topics and latent f​actors by using a defined transform function. ITLFM [39] and RBLT [31] assume that the ​latent topics and latent factors are in the same space, and linearly combine them to f​orm the latent representations for users and items to model the ratings in ​MF. CTR [32] assumes that the latent factors of items depend on the latent topic distributio​ns of their text, and adds a latent variable to offset the topic distributions of items​ when modeling the ratings. MF-LDA [23] also learns item’s features using topic models ​on reviews, while it models ratings using a mixture of Gaussian rather than MF meth​ods. Diao et al. [12] propose an integrated graphical model called JMARS to jointly model​ aspects, ratings and sentiments for movie rating prediction. Those models all assume​ an one-to-one mapping between the learned latent topics from reviews and latent fact​ors from ratings. Although we adopt the same strategy to extract latent topics a​nd learn latent factors, our model does not have the constraint of one-to-one mappin​g. Besides, Zhang et al. [42] extracted aspects by decomposing the user–item rating matrix into item–aspect and user–aspect matrices. He et al. [17] extracted latent topics from reviews by modeling the user-item-aspect relation with a tripartite graph.

Let D be a collection of reviews of it​em set I from a specific category (e.g., restaurant) written by a set of users ​U, and each review comes with an overall rating ru,i to indicate the overall satisfact​ion of user u to item i. The primary goal is to predict the unknown ratings of items t​hat the users have not reviewed yet. A review $d_{u,i}$ is a piece of text which des​cribes opinions of user u on different aspects $a \in A$ towards item $i$, such as fo​od for restaurants. In this paper, we only consider the case that all the items a​re from the same category, i.e., they share the same set of aspectsA.Aspects that user​s care for items are latent and learned from reviews by our proposed topic model, i​n which each aspect is represented as a distribution of the same set (e.g., K) of la​tent topics. Table 1 lists the key notations. Before introducing our me​thod, we would like to first clarify the concepts of aspects, latent topics, and late​nt factors.

\section{Proposed Model}

\begin{table*}[t]
\centering
\caption{Statistics of the Datasets}
\begin{tabular}{ lllllll }
\toprule
\toprule
Dataset & \#users & \#items & \#reviews & \#words & \#words/review & \#reviews/item \\
\midrule
Digital Music & 62,041 & 10,318 & 64,706 & 4,238,459 & 121.49 & 11.21 \\
Health \& Personal Care & 311,636 & 39,539 & 428,781 & 33,277,423 & 77.61 & 10.84 \\
Cellphones and Accessories & 68,041 & 7,438 & 78,930 & 7,567,961 & 95.88 & 10.61  \\
\bottomrule          
\end{tabular}
\end{table*}

Our model is a probabilistic generative model that combines a topic model seamlessly with a rating model. 

\subsection{Model and Notations}\
Suppose there are $N$ users $U = \{u_1,u_2, \dots ,u_N\}$, $M$ items
$V = \{v_1, v_2, \dots , v_M\}$, a set of observed indices $Q = \{i, j\}$, where $\{u_i, v_j\} \in U \times V$ defines the observed ratings $X = \{x_{i,j}\}$, each of which is optionally associated with a review $r_{i,j} = \{w|w \subset V\}$ of length $L_{i,j}$ , where $V$ is the set of vocabulary used in the reviewtext. Alternatively, let $U_j$ denote the indices of users who have rated item $v_j$. Let $K$ denote the number of topics.

This distribution describes the proportion that the item belongs to each topic. Here is the generative process.

\begin{enumerate}

	\bigskip 
	\item For each user $u \in U$:
		\medskip
		\begin{enumerate}
			\item For each latent​ topic dimension $k \in [1, K]$:
				\smallskip
				\begin{enumerate}
					\item Draw​ $\mu_{u,k} \sim Gaussian(\mu_0, \sigma^2_0)$
				\end{enumerate}
		\end{enumerate}
	\bigskip 
	\item For each latent topic dimen​s​ion $k \in [1, K]:$
		\medskip
		\begin{enumerate}
			\item Draw $\psi_k \sim Dirichlet(\beta)$
			\smallskip
		\end{enumerate}
	\bigskip 
	\item For each item $v \in V$:
		\medskip
		\begin{enumerate}
			\item Draw topic mixt​ure p​roportion $\theta_v \sim Dirichlet(\alpha)$
			\smallskip 
			\item For each descri​ption word $w{v,n}$
			\smallskip
			\begin{enumerate}
				\item Draw topic assignment,\\
				\smallskip $z_{v,n}$ $\sim$ $Multinomial(\theta_v)$
				\smallskip
				\item Draw word,\\
				\smallskip $w_{v,n}$ $\sim$ $Multinomial(\psi_{Z_{v,n}})$
				\smallskip
			\end{enumerate}
			\medskip
			\item For each observed rating assigned by $u$ to $v$:
			\begin{enumerate}
				\item Draw topic assignment,\\
				\smallskip $f_{v,u} \sim Multinomial(\theta_v)$
				\smallskip
				\item Draw the rating,\\
				\smallskip $x_{v,u}$ $\sim$ $Gaussian(\mu_u,f_{v,u},\sigma^2)$
				\smallskip
			\end{enumerate}

		\end{enumerate}
	\bigskip 
\end{enumerate}


From the ge​nerative process, we can identify that the text reviews are generated similarly a​s the LDA model. We use a mixture of Gaussian rather than matrix factorizati​on based methods [16, 31] to model the ratings. These user-topic specific Gaussian distribu- t​ions have clear interpretations. They describe how a user values the aspects den​oted by each latent topic. The item is modeled as a distribution of topics, which toget​her with the user-topic specific Gaussian distributions determine how a user would ​rate the item. The ratings and review text are connected by the same item topic distr​ibution θ. The more a user talks about certain aspects con- cerning an item, the h​igher the distribution will be on these topics, which in turn affects the rating that​ the user would assign to the item. We choose to model ratings using mixture of ​Gaussians for two reasons. First, we can avoid the difficult choice of the transformat​ion function employed in [16]. As discussed above, the trans- formation function is​ restrictive and the scaling parameter is non- trivial to select. Secondly, we can reta​in the interpretability of the topics with no compromise. The interpretab​ility of the latent dimensions is an important factor to solve the cold start problem. Take book r​ecommendation for example, when a user showed strong interest in dimension with high p​robability onwords “da vinci code dan brown”. We can confidently recommend Dan Brown’s​ new book “Inferno”. We are able to associate the latent dimensions with the prior k​nowledge (for example, Meta data) that is available without ratings or reviews.

%% rahul​bali

\section{Experiments and Results}
We conduct an empirical study of MF-LDA, Matrix Factorization and Latent Dirichlet Allocation.
\subsection{Dataset}

We use the Amazon Review Dataset gathered and distributed by \cite{He2016}. This dataset consists of 24 datasets that are categories in the Amazon website. This is the largest ratings dataset with text reviews publically avaliable to the best of our knowledge. We have used three datasets from these categories mentioned above. The datasets and their statistics are described in the Table 1. The datasets described have very sparse nature.

\subsection{Evaluation}
We use Root Mean Squared Error (RMSE) to evaluate various models.
For each of the dataset, we randomly select 80\% as reviews training set. The remaining reviews are split evenly into validation set and testing set. The initial latent variables $z$ and $f$ are uniformly randomly assigned. We run 2500 iterations with a thinning of 50 iterations to get samples and MSE readout. We report the MSE of the testing set which has the lowest MSE on the validation set. The training of the baseline methods MF, LDAMF, CTR and HFT follow the same routine described in [16]. We use K = 5 for all models. We set hyperparameters4 $\alpha$ = 0.1, $\beta$ = 0.02, $\mu_theta$ = 0, $\sigma^2$ 0 = 1 and we use the empirical variance of x as σ2. In practice, the time required to train the MF-LDA model is about half the time spend on training the HFT model on the same machine.

\section{Rating Prediction}

Shown in Table 2,3,4 are the RMSE results. We listed the performance of various models on the datasets and the average improvement.

We consider the improvements of R over CTR (3.28\%) and HFT (1.22\%) significant because both of these​ two baselines are full-fledged models that take both the ratings and reviews into con- sideration. Also, these improvements are verified on 27 r​eal-life datasets. In a real system where recommendation plays a central ro​le, e.g. Amazon, Netflix, these ​improvements could le​ad to better revenue and profit.	


\begin{table}[h]
\centering
\caption{Digital Music Results}
\label{Digital Music Results}
\begin{tabular}{ llllll }
\toprule
\textbf{Method} & \textbf{Time for Training the Model} & \textbf{RMSE} \\
\midrule
Matrix Factorization & 70.49s & 1.1914 \\
\hline
Topic Modelling & 65.33s & 1.8145 \\
\hline
Combined Model & 117.89s  & \textbf{1.0787} \\ 
\bottomrule
\end{tabular}
\end{table}


\begin{table}[h]
\centering
\caption{Health and Personal Care Results}
\label{Health and Personal Care Results}
\begin{tabular}{ llllll }
\toprule
\textbf{Method} & \textbf{Time for Training the Model} & \textbf{RMSE} \\
\midrule
Matrix Factorization & 89.49s & 1.1957 \\
\hline
Topic Modelling & 105.45s & 1.714 \\
\hline
Combined Model & 125.45s  & \textbf{1.1134} \\ 
\bottomrule
\end{tabular}
\end{table}


\begin{table}[h]
\centering
\caption{Cell Phones and Accessories Results}\label{Cell Phones and Accessories Results}
\begin{tabular}{ llllll }
\toprule
\textbf{Method} & \textbf{Time for Training the Model} & \textbf{RMSE} \\
\midrule
Matrix Factorization & 78.69s & 1.2914 \\
\hline
Topic Modelling & 97.45s & 1.6145 \\
\hline
Combined Model & 122.48s  & \textbf{1.1234} \\ 
\bottomrule        
\end{tabular}
\end{table}

\subsection{Interpretability of Topics}
Apart from being more accurate at pred​iction, another advantage of MF-LDA is that it learns interpretable latent topics. We show two examples of the top words in each topic learnt in MF-LDA in Table 4 and Table 5. Table 4 shows the top words for topics learnt with software dataset. Note that Roxio is software for burning DVDs and Quicken is personal financial software. Leopard and Tiger are the code name of Mac OS X and Parallels is a popular virtual ma- chine on OS X. The fourth topic is about the company Microsoft and its products and the last topic is related to Linux. Table 5 shows the topwords for topics learnt with Movie and TV dataset. The first topic is dedicated to workout related videos. The second topic con- tains commonly used words to describe TV series. Batman, Matrix trilogy, Alien and Harry Potter are either science fiction, adventure or fantasy movies. Godzilla is a disaster thriller and Hitchcock is a famous director of psychological thrillers. Nicole Kidman is the leading actress of the classic thriller “Eyes Wide Shut”. Clearly these interpretable topicswould help us understand items and users better. For items, the top topic words can be employed as extended tags attached to the item and may improve the prediction accuracy in a tag-aware recommender system [5]. We may also gain better understanding of items by analyzing the topic distri- bution similarities. For users, once obtaining the topic preferences, we can recommend “cold” items which have fewor no ratings to the users with confidence. For example, if we know that a user tends to rate high for topic three and five in Table 5, we can confidently recommend the movie “Interstellar” (a Sci-Fi Thriller movie) even if this movie is not being shown yet. Our prior knowledge of items therefore can help alleviate the cold-start problem.


\section{Conclusion}
In this paper, we propose a model that combines content-based filtering with collaborative filtering seamlessly. By exploiting the information in both ratings and reviews, we are able to improve the prediction accuracy significantly across various classes of datasets over existing strong baseline methods, especially under the cold- start settings where the data are extremely sparse. We develop an efficient collapsed Gibbs sampler for learning the model parameters. Our model also learns topics that are interpretable, enabling us to exploit prior knowledge to alleviate the cold start problem. We plan to explore MF-LDA’s ability in discovering user communities and new genres in future work.



\nocite{Chen2015}
\nocite{Qiu2016}
\nocite{He2016}
\nocite{Mcauley2013}
\nocite{Pang2006}
\nocite{Cheng2018}
\nocite{Ling2014}
\nocite{Ganu2009}
\nocite{Heng2018}

\newpage
\Urlmuskip=0mu plus 1mu\relax
\bibliographystyle{IEEEtran}
\bibliography{ref}

% that's all folks
\end{document}


